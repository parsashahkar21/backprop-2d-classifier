2D Neural Network Classifier (Backprop + GUI)

A from-scratch neural network implementation for classifying 2D data points, enhanced with a graphical interface for experimenting with training scenarios. The project includes manual backpropagation, multiple optimizers (Adam, Momentum, SGD), data generation controls, and real-time visualization of decision boundaries.

â¸»

ğŸš€ Features
	â€¢	Neural network from scratch (NumPy)
	â€¢	Manual backpropagation
	â€¢	Multiple optimizers:
	â€¢	Adam
	â€¢	Momentum
	â€¢	Standard Gradient Descent
	â€¢	Interactive GUI (Tkinter):
	â€¢	Visualize training in real time
	â€¢	Adjust dataset size and noise
	â€¢	Explore different training scenarios
	â€¢	Compare optimizer behaviors
	â€¢	2D data classification with decision boundary visualization
	â€¢	Cross-platform support (Windows / macOS / Linux)

â¸»

ğŸ–¥ï¸ Usage

Run the GUI instead of the example script:

python gui.py

The GUI allows you to:
	â€¢	Set number of data points
	â€¢	Add noise to the dataset
	â€¢	Select optimizer
	â€¢	Tune learning rate and other hyperparameters
	â€¢	Start/stop training live
	â€¢	Watch the decision boundary evolve in real time

â¸»

ğŸ“¦ Requirements
	â€¢	Python 3.8+
	â€¢	Tkinter (included in most Python installations)
	â€¢	NumPy
	â€¢	Matplotlib (if used for visualization)

â¸»

ğŸ macOS / Linux Note

On macOS or Linux, running the project inside a Conda environment is recommended, especially for Tkinter compatibility:

conda create -n nn2d python=3.10
conda activate nn2d


â¸»

ğŸ“ Project Structure

â”œâ”€â”€ gui.py              # Main GUI interface
â”œâ”€â”€ example.py          # Old example (use gui.py instead)
â”œâ”€â”€ nn/                 # Neural network implementation
â”œâ”€â”€ optimizers/         # Adam, Momentum, SGD implementations
â”œâ”€â”€ utils/              # Dataset generation, plotting, helpers
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»

ğŸ¯ Project Purpose

This project is designed for experimentation and learning. It provides insight into how neural networks train by allowing you to:
	â€¢	See gradients and decision boundaries update live
	â€¢	Compare optimizers like Adam and Momentum
	â€¢	Interactively control dataset complexity
	â€¢	Understand the impact of noise and sample size

